"""
GitHub Models AI í´ë¼ì´ì–¸íŠ¸
GitHubì—ì„œ í˜¸ìŠ¤íŒ…í•˜ëŠ” ë‹¤ì–‘í•œ AI ëª¨ë¸ ì‚¬ìš© (GPT-4o, Llama, Mistral, Phi ë“±)
"""
import os
from typing import Optional
from openai import OpenAI


class GitHubModelsClient:
    """GitHub Models API í´ë¼ì´ì–¸íŠ¸"""
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì™„ë£Œëœ ëª¨ë¸ë“¤ (2025ë…„ 6ì›” ê²€ì¦)
    AVAILABLE_MODELS = {
        # OpenAI ëª¨ë¸ (âœ… ê²€ì¦ë¨)
        "gpt-4o": "gpt-4o",
        "gpt-4o-mini": "gpt-4o-mini",
        "gpt-4.1": "gpt-4.1",
        "gpt-4.1-mini": "gpt-4.1-mini",
        "gpt-4.1-nano": "gpt-4.1-nano",
        
        # Meta Llama ëª¨ë¸ (âœ… ê²€ì¦ë¨)
        "llama-3.3-70b": "Llama-3.3-70B-Instruct",
        "llama-3.2-90b-vision": "Llama-3.2-90B-Vision-Instruct",
        
        # Microsoft Phi ëª¨ë¸ (âœ… ê²€ì¦ë¨)
        "phi-4": "Phi-4",
        "phi-4-mini": "Phi-4-mini-instruct",
        
        # DeepSeek ëª¨ë¸ (âœ… ê²€ì¦ë¨)
        "deepseek-r1": "DeepSeek-R1",
        "deepseek-r1-0528": "DeepSeek-R1-0528",
        
        # Mistral ëª¨ë¸ (âœ… ê²€ì¦ë¨)
        "codestral": "Codestral-2501",
    }
    
    def __init__(self, model: str = "gpt-4o-mini", token: str = None):
        """
        Args:
            model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-4o-mini)
            token: GitHub Personal Access Token (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.token = token or os.getenv("GITHUB_TOKEN") or os.getenv("GITHUB_PAT")
        
        if not self.token:
            raise ValueError(
                "GitHub Tokenì´ í•„ìš”í•©ë‹ˆë‹¤. "
                "GITHUB_TOKEN ë˜ëŠ” GITHUB_PAT í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ "
                "token íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•˜ì„¸ìš”."
            )
        
        # ëª¨ë¸ ì´ë¦„ ë§¤í•‘
        self.model = self.AVAILABLE_MODELS.get(model, model)
        self.model_key = model
        
        # OpenAI SDKë¥¼ GitHub Models ì—”ë“œí¬ì¸íŠ¸ë¡œ ì„¤ì •
        self.client = OpenAI(
            base_url="https://models.github.ai/inference",  # ì˜¬ë°”ë¥¸ GitHub Models ì—”ë“œí¬ì¸íŠ¸
            api_key=self.token
        )
    
    def chat(self, 
             messages: list,
             temperature: float = 0.7,
             max_tokens: int = 4000,
             stream: bool = False) -> str:
        """
        ì±„íŒ… ì™„ì„± API í˜¸ì¶œ
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user", "content": "..."}]
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (0-1)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
            
        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )
            
            if stream:
                return response  # ìŠ¤íŠ¸ë¦¼ ê°ì²´ ë°˜í™˜
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"GitHub Models API ì˜¤ë¥˜: {str(e)}"
    
    def analyze(self, prompt: str, system_prompt: str = None) -> str:
        """
        ê°„ë‹¨í•œ ë¶„ì„ ìš”ì²­
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            
        Returns:
            AI ì‘ë‹µ
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        return self.chat(messages)
    
    @classmethod
    def list_models(cls) -> dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return cls.AVAILABLE_MODELS
    
    @classmethod
    def get_model_info(cls) -> str:
        """ëª¨ë¸ ì •ë³´ ë¬¸ìì—´ ë°˜í™˜"""
        info = "ğŸ“‹ GitHub Models ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸:\n\n"
        
        categories = {
            "OpenAI": ["gpt-4o", "gpt-4o-mini"],
            "Meta Llama": ["llama-3.1-405b", "llama-3.1-70b", "llama-3.1-8b", "llama-3.2-90b", "llama-3.2-11b"],
            "Mistral": ["mistral-large", "mistral-small", "mistral-nemo"],
            "Microsoft Phi": ["phi-4", "phi-3.5-mini", "phi-3.5-moe"],
            "Cohere": ["cohere-command-r", "cohere-command-r-plus"],
            "AI21": ["jamba-1.5-large", "jamba-1.5-mini"]
        }
        
        for category, models in categories.items():
            info += f"**{category}**\n"
            for model in models:
                info += f"  - {model}\n"
            info += "\n"
        
        return info


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print(GitHubModelsClient.get_model_info())
    
    # í† í°ì´ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸
    try:
        client = GitHubModelsClient(model="gpt-4o-mini")
        response = client.analyze("í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?")
        print(f"\ní…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response}")
    except ValueError as e:
        print(f"\nâš ï¸ {e}")
